{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46406998",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# import pyvista as pv\n",
    "from collections import defaultdict, deque\n",
    "import copy\n",
    "\n",
    "# ----------- Normal vectors estimation functions -----------\n",
    "\n",
    "def compute_face_normals_and_areas(mesh):\n",
    "    \"\"\"\n",
    "    Computes normal and area for each triangle face in the mesh.\n",
    "\n",
    "    Returns:\n",
    "        normals (np.ndarray): An (F, 3) array of unit normal vectors for each face.\n",
    "        areas (np.ndarray): A (F,) array of areas for each face.\n",
    "\n",
    "    Method:\n",
    "        - Extract vertex indices for each triangle face.\n",
    "        - Compute two edge vectors of the triangle.\n",
    "        - Use cross product to get the face normal (not normalized).\n",
    "        - Normalize to get unit normals.\n",
    "        - Compute triangle area as half of the magnitude of the cross product.\n",
    "    \"\"\"\n",
    "    faces = mesh.faces.reshape((-1, 4))[:, 1:]\n",
    "    v0 = mesh.points[faces[:, 0]]\n",
    "    v1 = mesh.points[faces[:, 1]]\n",
    "    v2 = mesh.points[faces[:, 2]]\n",
    "    \n",
    "    cross = np.cross(v1 - v0, v2 - v0)\n",
    "    areas = 0.5 * np.linalg.norm(cross, axis=1)\n",
    "    normals = cross / np.maximum(np.linalg.norm(cross, axis=1, keepdims=True), 1e-8)\n",
    "    return normals, areas\n",
    "\n",
    "def build_vertex_to_faces_map(mesh):\n",
    "    \"\"\"\n",
    "    Builds a mapping from each vertex index to the list of face indices that include that vertex.\n",
    "\n",
    "    The same with pyvista's mesh.point_cell_ids()\n",
    "    Returns:\n",
    "        defaultdict(list): A dictionary where each key is a vertex index, and the value is \n",
    "        a list of face indices (from mesh.faces) that contain the vertex.\n",
    "\n",
    "    Example:\n",
    "        {\n",
    "            0: {1, 5, 10},  # vertex 0 is part of faces 1, 5, and 10\n",
    "            1: {0, 2},      # vertex 1 is part of faces 0 and 2\n",
    "            ...\n",
    "        }    \n",
    "    \"\"\"\n",
    "    vertex_faces = defaultdict(list)\n",
    "    faces = mesh.faces.reshape((-1, 4))[:, 1:]\n",
    "    for i, (v1, v2, v3) in enumerate(faces):\n",
    "        vertex_faces[v1].append(i)\n",
    "        vertex_faces[v2].append(i)\n",
    "        vertex_faces[v3].append(i)\n",
    "    return vertex_faces\n",
    "\n",
    "def compute_area_weighted_vertex_normals(mesh):\n",
    "    \"\"\"\n",
    "    Computes unit vertex normals by averaging adjacent face normals, weighted by face area.\n",
    "\n",
    "    Returns:\n",
    "        vertex_normals (np.ndarray): An (N, 3) array of unit normal vectors for each vertex.\n",
    "\n",
    "    Method:\n",
    "        1. Compute face normals and face areas using cross products.\n",
    "        2. Build a mapping from each vertex to the list of adjacent face indices.\n",
    "        3. For each vertex:\n",
    "            - Retrieve adjacent faces.\n",
    "            - Average their normals, weighted by face area.\n",
    "            - Normalize the result to obtain a unit normal.\n",
    "    \"\"\"\n",
    "    face_normals, face_areas = compute_face_normals_and_areas(mesh)\n",
    "    vertex_faces = build_vertex_to_faces_map(mesh)\n",
    "    num_vertices = mesh.points.shape[0]\n",
    "    vertex_normals = np.zeros((num_vertices, 3))\n",
    "\n",
    "    for vi in range(num_vertices):\n",
    "        adjacent_faces = vertex_faces[vi]\n",
    "        if not adjacent_faces:\n",
    "            continue\n",
    "        areas = face_areas[adjacent_faces]\n",
    "        normals = face_normals[adjacent_faces]\n",
    "        weighted_sum = np.sum(normals * areas[:, np.newaxis], axis=0)\n",
    "        vertex_normals[vi] = weighted_sum / np.linalg.norm(weighted_sum)\n",
    "    \n",
    "    return vertex_normals\n",
    "\n",
    "# -------------------------- Normal vector angle --------------\n",
    "\n",
    "def normal_vector_angle(n_p, n_q, degrees=False):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    - n_p, n_q: 3D vectors (can be non-normalized).\n",
    "    - degrees: If True, return angle in degrees. Otherwise, radians.\n",
    "\n",
    "    Returns:\n",
    "    - Angle between vectors\n",
    "    \"\"\"\n",
    "    dot_product = np.dot(n_p, n_q)\n",
    "    norm_p = np.linalg.norm(n_p)\n",
    "    norm_q = np.linalg.norm(n_q)\n",
    "\n",
    "    if norm_p == 0 or norm_q == 0:\n",
    "        return 0.0\n",
    "\n",
    "    cos_theta = np.clip(dot_product / (norm_p * norm_q), -1.0, 1.0)\n",
    "    angle_rad = np.arccos(cos_theta)\n",
    "    return np.degrees(angle_rad) if degrees else angle_rad\n",
    "\n",
    "# ----------------------------------------------\n",
    "def sorting_flat_vertices_with_flatness(normals, adjacency):\n",
    "    \"\"\"\n",
    "    Find all flat vertices, compute their flatness (average normal angle to neighbors),\n",
    "    and return them sorted by flatness (ascending).\n",
    "    \n",
    "    Returns:\n",
    "        List of (vertex_index, flatness_value) tuples, sorted by flatness_value.\n",
    "    \"\"\"\n",
    "    flat_vertices = []\n",
    "    for vi, neighbors in adjacency.items():\n",
    "        if not neighbors:\n",
    "            continue\n",
    "        # Check if all neighbor angles are below threshold\n",
    "        angles = [normal_vector_angle(normals[vi], normals[vj]) for vj in neighbors]\n",
    "        flatness = sum(angles) / len(angles)\n",
    "        flat_vertices.append((vi, flatness))\n",
    "        # if all(angle < threshold for angle in angles):\n",
    "        #     flatness = sum(angles) / len(angles)\n",
    "        #     flat_vertices.append((vi, flatness))\n",
    "        \n",
    "    # Sort by flatness value (ascending)\n",
    "    flat_vertices.sort(key=lambda x: x[1])\n",
    "    return flat_vertices\n",
    "\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "def cluster_surface(adj, v_normals, flat_v_sorted, weight, threshold):\n",
    "    flat_v_sorted_copy = deque(flat_v_sorted)  # Faster .popleft()\n",
    "    unvisited = set(range(len(v_normals)))\n",
    "    clusters = []\n",
    "\n",
    "    def compute_avg_angle(normal, neighbors):\n",
    "        return np.mean([\n",
    "            normal_vector_angle(normal, v_normals[nj])\n",
    "            for nj in neighbors\n",
    "        ])\n",
    "\n",
    "    while unvisited:\n",
    "        while flat_v_sorted_copy:\n",
    "            start = flat_v_sorted_copy.popleft()\n",
    "            if start in unvisited:\n",
    "                break\n",
    "        else:\n",
    "            break  # No unvisited nodes left\n",
    "\n",
    "        cluster = set([start])\n",
    "        queue = deque([start])\n",
    "        unvisited.remove(start)\n",
    "\n",
    "        # Maintain running sum of normals for this cluster\n",
    "        normals_sum = v_normals[start].copy()\n",
    "\n",
    "        while queue:\n",
    "            vi = queue.popleft()\n",
    "            for vj in adj[vi]:\n",
    "                if vj not in unvisited:\n",
    "                    continue\n",
    "\n",
    "                # Average angle between vj and its neighbors\n",
    "                avg_angle_vi = compute_avg_angle(v_normals[vi], adj[vj])\n",
    "\n",
    "                # Compute updated cluster normal\n",
    "                cluster_normal = normals_sum / np.maximum(np.linalg.norm(normals_sum), 1e-8)\n",
    "\n",
    "                avg_angle_cluster = compute_avg_angle(cluster_normal, adj[vj])\n",
    "                proximity = weight * avg_angle_vi + (1 - weight) * avg_angle_cluster\n",
    "\n",
    "                if proximity < threshold:\n",
    "                    cluster.add(vj)\n",
    "                    queue.append(vj)\n",
    "                    unvisited.remove(vj)\n",
    "                    normals_sum += v_normals[vj]  # Update running normal sum\n",
    "\n",
    "        clusters.append(cluster)\n",
    "\n",
    "    return clusters\n",
    "\n",
    "\n",
    "def merge_noise_faces(clusters, vertex_normals, adjacency, K=10):\n",
    "    \"\"\"\n",
    "    Merge small (noise) clusters into neighboring real clusters based on normal similarity,\n",
    "    while avoiding merging adjacent noise clusters together.\n",
    "\n",
    "    Parameters:\n",
    "        clusters (List[Set[int]]): List of vertex index sets (each cluster).\n",
    "        vertex_normals (np.ndarray): (N, 3) array of vertex normals.\n",
    "        adjacency (dict): {vertex_index: set(neighboring vertex indices)}.\n",
    "        K (int): Max vertex count for a cluster to be considered noise.\n",
    "\n",
    "    Returns:\n",
    "        List[Set[int]]: Updated clusters with noise surfaces merged into real surfaces.\n",
    "    \"\"\"\n",
    "    # Separate clusters\n",
    "    real_clusters = [c for c in clusters if len(c) >= K]\n",
    "    noise_clusters = [c for c in clusters if len(c) < K]\n",
    "\n",
    "    # Precompute normals\n",
    "    def cluster_avg_normal(cluster):\n",
    "        normals = [vertex_normals[vi] for vi in cluster]\n",
    "        summed = np.sum(normals, axis=0)\n",
    "        return summed / np.linalg.norm(summed) if np.linalg.norm(summed) > 0 else summed\n",
    "\n",
    "    real_normals = [cluster_avg_normal(c) for c in real_clusters]\n",
    "    noise_normals = [cluster_avg_normal(c) for c in noise_clusters]\n",
    "\n",
    "    # Create index-to-cluster maps for fast lookup\n",
    "    vertex_to_real_cluster = {}\n",
    "    for idx, cluster in enumerate(real_clusters):\n",
    "        for v in cluster:\n",
    "            vertex_to_real_cluster[v] = idx\n",
    "\n",
    "    merged_indices = set()  # track merged noise cluster indices\n",
    "\n",
    "    for ni, noise in enumerate(noise_clusters):\n",
    "        if ni in merged_indices:\n",
    "            continue\n",
    "        noise_normal = noise_normals[ni]\n",
    "\n",
    "        # Find adjacent real clusters only\n",
    "        neighbor_real_candidates = set()\n",
    "        for vi in noise:\n",
    "            for vj in adjacency[vi]:\n",
    "                if vj in vertex_to_real_cluster:\n",
    "                    neighbor_real_candidates.add(vertex_to_real_cluster[vj])\n",
    "\n",
    "        # Find the best matching real cluster by normal vector angle\n",
    "        best_real_idx = None\n",
    "        best_angle = float('inf')\n",
    "\n",
    "        for r_idx in neighbor_real_candidates:\n",
    "            angle = normal_vector_angle(noise_normal, real_normals[r_idx])\n",
    "            if angle < best_angle:\n",
    "                best_real_idx = r_idx\n",
    "                best_angle = angle\n",
    "\n",
    "        # Merge into best-matching real cluster\n",
    "        if best_real_idx is not None:\n",
    "            real_clusters[best_real_idx].update(noise)\n",
    "            merged_indices.add(ni)\n",
    "        # else: discard the noise cluster (could also be kept separately)\n",
    "\n",
    "    return real_clusters\n",
    "\n",
    "\n",
    "'''\n",
    "    This code implement the Laplace smoothing algorithm mentioned in the section 2 of the paper \n",
    "    \"Robust surface segmentation and edge feature lines extraction from fractured\n",
    "    ragments of relics\" \n",
    "'''\n",
    "\n",
    "\n",
    "# ---------- Laplacian Smoothing Functions ----------\n",
    "\n",
    "def build_vertex_adjacency(faces):\n",
    "    adjacency = defaultdict(set)\n",
    "    for i, j, k in faces:\n",
    "        adjacency[i].update((j, k))\n",
    "        adjacency[j].update((i, k))\n",
    "        adjacency[k].update((i, j))\n",
    "    return adjacency\n",
    "\n",
    "def weighted_laplacian_smoothing(vertices, faces, iterations=1):\n",
    "    vertices = vertices.copy()\n",
    "    adjacency = build_vertex_adjacency(faces)\n",
    "\n",
    "    for _ in range(iterations):\n",
    "        new_vertices = vertices.copy()\n",
    "        for i in range(vertices.shape[0]):\n",
    "            neighbors = list(adjacency[i])\n",
    "            if not neighbors:\n",
    "                continue\n",
    "\n",
    "            neighbors_coords = vertices[neighbors]  # shape: (N_neighbors, 3)\n",
    "            diffs = neighbors_coords - vertices[i]  # shape: (N_neighbors, 3)\n",
    "            dists = np.linalg.norm(diffs, axis=1) + 1e-8  # avoid zero division\n",
    "            weights = 1.0 / dists\n",
    "            weights /= np.sum(weights)\n",
    "\n",
    "            weighted_sum = np.dot(weights, diffs)  # shape: (3,)\n",
    "            new_vertices[i] += weighted_sum\n",
    "\n",
    "        vertices = new_vertices\n",
    "\n",
    "    return vertices\n",
    "\n",
    "def compute_mesh_smoothness(vertices, faces): # To see how much a mesh is smooth\n",
    "    adjacency = build_vertex_adjacency(faces)\n",
    "    smoothness_vals = []\n",
    "\n",
    "    for i in range(len(vertices)):\n",
    "        neighbors = list(adjacency[i])\n",
    "        if not neighbors:\n",
    "            continue\n",
    "        avg_neighbor = np.mean(vertices[neighbors], axis=0)\n",
    "        smoothness = np.linalg.norm(vertices[i] - avg_neighbor)\n",
    "        smoothness_vals.append(smoothness)\n",
    "\n",
    "    return np.mean(smoothness_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67092b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement open3d (from versions: none)\n",
      "ERROR: No matching distribution found for open3d\n"
     ]
    }
   ],
   "source": [
    "!pip install open3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bcf475f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_builtin_int(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        return {convert_to_builtin_int(k): convert_to_builtin_int(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_to_builtin_int(i) for i in obj]\n",
    "    elif isinstance(obj, set):\n",
    "        return {convert_to_builtin_int(i) for i in obj}\n",
    "    elif isinstance(obj, tuple):\n",
    "        return tuple(convert_to_builtin_int(i) for i in obj)\n",
    "    elif isinstance(obj, np.integer):  # Catch np.int64, np.int32, ...\n",
    "        return int(obj)\n",
    "    else:\n",
    "        return obj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9328c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pyvista as pv\n",
    "\n",
    "mesh = pv.read('D:/Learn_and_Study/USTH/Bachelor/3D_Project/CG_dataset/brick_part01.obj')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "334092ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "faces_1D = mesh.faces\n",
    "faces_nD = faces_1D.reshape((-1, 4))[:, 1:]\n",
    "\n",
    "smoothed_vertices = weighted_laplacian_smoothing(mesh.points, faces_nD, 3)\n",
    "smoothed_mesh = pv.PolyData(smoothed_vertices, faces_1D)\n",
    "\n",
    "import gc\n",
    "del mesh\n",
    "del smoothed_vertices\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56d2653e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "vertices = smoothed_mesh.points \n",
    "faces_1D = smoothed_mesh.faces\n",
    "faces_nD = smoothed_mesh.faces.reshape((-1, 4))[:, 1:]\n",
    "\n",
    "del faces_1D\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23db0ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "adj = build_vertex_adjacency(faces_nD)\n",
    "adj = convert_to_builtin_int(adj)\n",
    "\n",
    "v_normals = compute_area_weighted_vertex_normals(smoothed_mesh)\n",
    "\n",
    "flat_v_sorted = sorting_flat_vertices_with_flatness(v_normals, adj)\n",
    "flat_v_sorted = [vertex for vertex, flatness in flat_v_sorted]\n",
    "flat_v_sorted = convert_to_builtin_int(flat_v_sorted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1ccb0a",
   "metadata": {},
   "source": [
    "### Test cluster_surface()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eee7ff0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = cluster_surface(adj, v_normals, flat_v_sorted, 0.5, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8df7a0",
   "metadata": {},
   "source": [
    "### Test merge_noise_face() to remove noise faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e5eda6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = merge_noise_faces(clusters, v_normals, adj, K=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fca1de99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05cecc7afb544a3bb5ce5354da8a7478",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Widget(value='<iframe src=\"http://localhost:52339/index.html?ui=P_0x17900b5a490_3&reconnect=auto\" class=\"pyvis‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colors = [\n",
    "    'red', 'green', 'blue', 'yellow', 'magenta', 'cyan',\n",
    "    'orange', 'lime', 'deeppink', 'deepskyblue', 'gold',\n",
    "    'indigo', 'teal', 'crimson', 'mediumvioletred',\n",
    "    'chartreuse', 'orangered', 'darkturquoise',\n",
    "    'slateblue', 'darkgoldenrod'\n",
    "]\n",
    "\n",
    "plotter = pv.Plotter(shape=(1, 3))\n",
    "\n",
    "plotter.subplot(0, 0)\n",
    "plotter.add_title(\"Original\")\n",
    "plotter.add_mesh(smoothed_mesh, show_edges=True)\n",
    "\n",
    "plotter.subplot(0, 1)\n",
    "plotter.add_title(\"Clusters\")\n",
    "plotter.add_mesh(smoothed_mesh, show_edges=True)\n",
    "for i, cluster in enumerate(clusters):\n",
    "    cluster_indices = list(cluster)\n",
    "    color = colors[i % len(colors)]\n",
    "    plotter.add_mesh(smoothed_mesh.points[cluster_indices], color=color, point_size=10)\n",
    "\n",
    "\n",
    "plotter.subplot(0, 2)\n",
    "plotter.add_title(\"Merged\")\n",
    "plotter.add_mesh(smoothed_mesh, show_edges=True)\n",
    "for i, cluster in enumerate(merged):\n",
    "    merged_indices = list(cluster)\n",
    "    color = colors[i % len(colors)]\n",
    "    plotter.add_mesh(smoothed_mesh.points[merged_indices], color=color, point_size=10)\n",
    "\n",
    "plotter.link_views()\n",
    "plotter.show()\n",
    "\n",
    "import gc\n",
    "del colors, color, i, plotter, cluster_indices, merged_indices, cluster\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b89532",
   "metadata": {},
   "source": [
    "### Test surface differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57eed03",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_indices = list(merged[0])[:50] + list(merged[2])[:50]\n",
    "fractured_indices = list(merged[5])[:50] + list(merged[4])[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0292560a",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_original_indices = np.array([32768, 32770, 32771, 32772, 32792, 32795, 32829, 32833, 32838, 32840, 32848, 32875, 32878, 32886, 185, 32974, 32976, 32978, 32988, 32991, 32996, 33002, 33005, 33008, 33010, 33022, 33025, 33027, 33034, 33058, 33061, 33064, 33067, 33091, 33093, 33095, 33097, 33100, 33110, 33114, 33125, 33208, 33210, 33214, 33218, 33224, 33227, 33228, 463, 465, 16384, 0, 1, 16387, 5, 7, 8, 1630, 11, 15, 18, 20, 21, 23, 26, 27, 28, 32, 37, 38, 40, 8234, 42, 46, 8239, 8244, 53, 8250, 59, 60, 62, 66, 67, 68, 8263, 71, 73, 74, 76, 77, 8270, 78, 79, 8278, 86, 8282, 93, 95, 96, 8293])\n",
    "training_fractured_indices = np.array([32774, 32775, 32777, 32778, 32782, 32783, 32785, 32787, 32788, 32789, 32791, 32796, 32798, 32799, 32801, 32802, 32803, 32804, 32806, 32807, 32808, 32809, 32812, 32814, 32815, 32819, 32822, 32823, 32824, 32827, 32830, 32834, 32836, 32841, 32843, 32845, 32847, 32850, 32853, 32857, 32858, 32859, 32861, 32863, 32864, 32867, 32868, 32869, 32876, 32880, 32769, 32773, 32776, 32779, 32780, 32781, 32784, 32786, 32790, 32793, 32794, 32797, 32805, 32810, 32813, 32816, 32817, 32818, 32820, 32821, 32825, 32826, 32828, 32831, 32832, 32835, 32837, 32839, 32842, 32844, 32846, 32849, 32851, 32852, 32854, 32855, 32856, 32860, 32862, 32865, 32866, 32870, 32871, 32872, 32873, 32874, 32877, 32879, 32881, 32885])\n",
    "\n",
    "test_original_indices = np.array([32770, 32773, 32778, 32781, 32786, 32790, 32791, 32792, 32798, 32799, 32800, 32880, 32933, 32938, 32946, 32952, 33004, 33007, 33010, 33014, 33016, 33019, 33021, 33039, 33044, 33054, 33056, 33060, 33070, 33094, 33097, 33125, 33135, 33153, 33156, 33161, 33163, 33189, 33191, 33194, 33198, 33199, 33202, 33205, 33208, 33209, 33212, 33214, 33216, 33218, 32769, 24578, 24580, 20485, 18438, 32774, 32777, 20493, 20495, 32783, 10255, 20498, 18451, 10258, 10260, 20502, 22550, 20504, 10263, 18458, 24603, 20507, 18460, 20510, 10266, 10270, 18466, 28710, 18471, 18473, 24618, 18474, 20524, 28714, 18478, 28718, 30769, 28722, 18485, 18486, 30774, 18488, 12343, 18490, 28730, 18492, 24637, 28733, 24640, 28736])\n",
    "test_fractured_indices = np.array([6513, 32772, 32775, 32776, 32779, 32780, 32782, 32784, 32785, 32787, 32788, 32789, 32795, 32801, 34, 32803, 33, 32805, 32806, 32807, 32808, 32809, 32810, 32811, 32812, 32813, 32814, 32815, 32816, 32817, 32818, 32819, 32820, 32821, 32822, 32823, 32824, 32825, 32826, 32827, 32828, 32829, 32830, 32831, 32832, 32833, 32834, 32835, 32836, 32837, 32768, 32771, 32793, 32794, 32796, 32843, 32845, 32846, 32847, 32848, 32849, 32850, 32851, 32852, 32853, 32854, 32855, 32856, 32857, 32858, 32859, 32861, 32864, 32868, 32869, 32870, 32872, 32874, 32875, 32876, 32878, 32882, 32883, 32884, 32885, 32886, 32888, 32889, 32891, 32894, 32895, 32896, 32897, 32898, 32899, 32900, 32901, 32902, 32903, 32904])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fc467545",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_local_bending_energy(vertices, vertex_normals, adjacency):\n",
    "    \"\"\"\n",
    "    Computes local bending energy e_k(p) for each vertex p.\n",
    "\n",
    "    Parameters:\n",
    "        vertices (np.ndarray): (N, 3) array of vertex positions.\n",
    "        vertex_normals (np.ndarray): (N, 3) array of vertex normals.\n",
    "        adjacency (dict): {vertex_index: set of neighbor vertex indices}\n",
    "\n",
    "    Returns:\n",
    "        e_k (np.ndarray): (N,) array of local bending energy per vertex.\n",
    "    \"\"\"\n",
    "    N = len(vertices)\n",
    "    e_k = np.zeros(N)\n",
    "\n",
    "    for p, neighbors in adjacency.items():\n",
    "        if not neighbors:\n",
    "            continue\n",
    "\n",
    "        v_p = vertices[p]\n",
    "        n_p = vertex_normals[p]\n",
    "\n",
    "        v_neighbors = vertices[list(neighbors)]\n",
    "        n_neighbors = vertex_normals[list(neighbors)]\n",
    "\n",
    "        # Vector differences\n",
    "        norm_diffs = np.sum((n_p - n_neighbors) ** 2, axis=1)\n",
    "        geom_dists = np.sum((v_p - v_neighbors) ** 2, axis=1) + 1e-8  # prevent divide by 0\n",
    "\n",
    "        e_k[p] = np.mean(norm_diffs / geom_dists)\n",
    "\n",
    "    return e_k\n",
    "\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "from scipy.spatial import cKDTree\n",
    "import numpy as np\n",
    "\n",
    "def compute_integrated_bending_energy_fast(e_k, vertices, radius):\n",
    "    \"\"\"\n",
    "    RAM-efficient version of computing e_{k,r}(p).\n",
    "    Parameters:\n",
    "        e_k (np.ndarray): (N,) local bending energy per vertex.\n",
    "        vertices (np.ndarray): (N, 3) coordinates.\n",
    "        radius (float): Radius for neighborhood averaging.\n",
    "    Returns:\n",
    "        e_kr (np.ndarray): (N,) integrated bending energy.\n",
    "    \"\"\"\n",
    "    N = len(vertices)\n",
    "    e_kr = np.empty(N, dtype=e_k.dtype)  # Use empty for performance\n",
    "    tree = cKDTree(vertices)\n",
    "\n",
    "    for i in range(N):\n",
    "        neighbors = tree.query_ball_point(vertices[i], r=radius)\n",
    "        if neighbors:\n",
    "            e_kr[i] = np.mean(e_k[neighbors])\n",
    "        else:\n",
    "            e_kr[i] = e_k[i]  # fallback\n",
    "\n",
    "    return e_kr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "81e59470",
   "metadata": {},
   "outputs": [],
   "source": [
    "e_k = compute_local_bending_energy(vertices, v_normals, adj)\n",
    "e_k = e_k.astype(np.float32)\n",
    "\n",
    "radius = 0.01\n",
    "e_kr = compute_integrated_bending_energy_fast(e_k, vertices, radius)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dd207092",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bending_energy_statistics(e_kr, original_indices, fractured_indices):\n",
    "    e_original = e_kr[original_indices]\n",
    "    e_fractured = e_kr[fractured_indices]\n",
    "\n",
    "    stats = {\n",
    "        'original_mean': np.mean(e_original),\n",
    "        'original_std': np.std(e_original),\n",
    "        'fractured_mean': np.mean(e_fractured),\n",
    "        'fractured_std': np.std(e_fractured),\n",
    "        'original_values': e_original,\n",
    "        'fractured_values': e_fractured\n",
    "    }\n",
    "    return stats\n",
    "\n",
    "def classify_vertices_by_threshold(e_kr, threshold):\n",
    "    return e_kr > threshold  # True = fracture, False = original\n",
    "\n",
    "def evaluate_classification(e_kr, labels, threshold):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    - e_kr: integrated bending energy\n",
    "    - labels: array where 1=fractured, 0=original\n",
    "    \"\"\"\n",
    "    preds = classify_vertices_by_threshold(e_kr, threshold).astype(int)\n",
    "    correct = np.sum(preds == labels)\n",
    "    accuracy = correct / len(labels)\n",
    "    return accuracy, preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5c954f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_radius(vertices, e_k, original_indices, fractured_indices, radius_list):\n",
    "    best_radius = None\n",
    "    best_accuracy = 0\n",
    "    best_stats = None\n",
    "\n",
    "    for r in radius_list:\n",
    "        e_kr = compute_integrated_bending_energy_fast(e_k, vertices, r)\n",
    "        stats = compute_bending_energy_statistics(e_kr, original_indices, fractured_indices)\n",
    "        threshold = (stats['original_mean'] + stats['fractured_mean']) / 2\n",
    "\n",
    "        labels = np.zeros(len(e_kr), dtype=int)\n",
    "        labels[original_indices] = 0\n",
    "        labels[fractured_indices] = 1\n",
    "\n",
    "        accuracy, _ = evaluate_classification(e_kr, labels, threshold)\n",
    "        print(f\"Radius: {r:.4f} | Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_radius = r\n",
    "            best_stats = stats\n",
    "\n",
    "    return best_radius, best_stats, best_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "174378fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111.09135\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(e_kr[test_fractured_indices]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806a077b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create labels for training set\n",
    "N = len(e_kr)\n",
    "labels = np.zeros(N, dtype=int)\n",
    "labels[training_fractured_indices] = 1\n",
    "labels[training_original_indices] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "773c05e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original mean: 163.092957\n",
      "Fractured mean: 111.091347\n",
      "Decision threshold: 137.092148\n"
     ]
    }
   ],
   "source": [
    "stats = compute_bending_energy_statistics(e_kr, original_indices, fractured_indices)\n",
    "\n",
    "# Decision threshold = midpoint between means\n",
    "threshold = (stats['original_mean'] + stats['fractured_mean']) / 2\n",
    "\n",
    "print(f\"Original mean: {stats['original_mean']:.6f}\")\n",
    "print(f\"Fractured mean: {stats['fractured_mean']:.6f}\")\n",
    "print(f\"Decision threshold: {threshold:.6f}\")\n",
    "\n",
    "# Predict: fractured if energy > threshold\n",
    "predictions = classify_vertices_by_threshold(e_kr, threshold)  # boolean mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f951b8c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training classification accuracy: 0.9070\n"
     ]
    }
   ],
   "source": [
    "# Build ground-truth label array (0 = original, 1 = fractured)\n",
    "labels = np.zeros_like(e_kr, dtype=int)\n",
    "labels[fractured_indices] = 1\n",
    "labels[original_indices] = 0\n",
    "\n",
    "accuracy, preds = evaluate_classification(e_kr, labels, threshold)\n",
    "print(f\"Training classification accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e1f89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get the index of the face color\n",
    "\n",
    "# # Fractured surface\n",
    "# magenta_index = colors.index('magenta')  # 4\n",
    "# magenta_points = list(clusters[magenta_index]) if magenta_index < len(clusters) else []\n",
    "\n",
    "# cyan_index = colors.index('cyan')  # 4\n",
    "# cyan_points = list(clusters[cyan_index]) if cyan_index < len(clusters) else []\n",
    "\n",
    "# # Original surface\n",
    "# red_index = colors.index('red')  # 4\n",
    "# red_points = list(clusters[red_index]) if red_index < len(clusters) else []\n",
    "\n",
    "# blue_index = colors.index('blue')  # 4\n",
    "# blue_points = list(clusters[blue_index]) if blue_index < len(clusters) else []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5cb678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(magenta_points))\n",
    "# print(len(cyan_points))\n",
    "# print(len(red_points))\n",
    "# print(len(blue_points))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14716a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# original_indices = red_points[:50] + blue_points[:50]\n",
    "# fractured_indices = cyan_points[:50] + magenta_points[:50]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceefe8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_values = e_kr[original_indices]\n",
    "fractured_values = e_kr[fractured_indices]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(original_values, bins=30, alpha=0.6, label='Original')\n",
    "plt.hist(fractured_values, bins=30, alpha=0.6, label='Fractured')\n",
    "plt.axvline(x=np.mean(original_values), color='blue', linestyle='--')\n",
    "plt.axvline(x=np.mean(fractured_values), color='red', linestyle='--')\n",
    "plt.legend()\n",
    "plt.title(\"Distribution of e_{k,r}(p)\")\n",
    "plt.xlabel(\"Integrated Bending Energy\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c134edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ch·ªçn ng∆∞·ª°ng t·ª± ƒë·ªông d·ª±a tr√™n gi√° tr·ªã trung b√¨nh\n",
    "threshold = (np.mean(original_values) + np.mean(fractured_values)) / 2\n",
    "\n",
    "# G√°n nh√£n: 0 = original, 1 = fractured\n",
    "vertex_labels = (e_kr > threshold).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b2fe2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "face_labels = []\n",
    "for face in faces_nD:\n",
    "    vertex_idxs = face\n",
    "    fractured_count = np.sum(vertex_labels[vertex_idxs])\n",
    "    if fractured_count / len(vertex_idxs) >= 0.6:  # > 50% l√† g√£y\n",
    "        face_labels.append(1)\n",
    "    else:\n",
    "        face_labels.append(0)\n",
    "\n",
    "face_labels = np.array(face_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc889150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# T·∫°o unstructured grid m·ªõi ƒë·ªÉ t√¥ m√†u theo m·∫∑t\n",
    "segmented_mesh = pv.PolyData(vertices, smoothed_mesh.faces)\n",
    "segmented_mesh.cell_data['surface_type'] = face_labels\n",
    "\n",
    "# Hi·ªÉn th·ªã\n",
    "plotter = pv.Plotter()\n",
    "plotter.add_mesh(segmented_mesh, scalars='surface_type', cmap='coolwarm', show_edges=True)\n",
    "plotter.add_legend([('Original', 'b'), ('Fractured', 'r')])\n",
    "plotter.add_title(\"Surface Classification (Original vs Fractured)\")\n",
    "plotter.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de634014",
   "metadata": {},
   "source": [
    "### Test find_cluster_edge_points()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c5dcf4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_cluster_edge_points(clusters, adj):\n",
    "    \"\"\"\n",
    "    Identify edge points in each cluster based on adjacency.\n",
    "\n",
    "    Args:\n",
    "        clusters (List[Set[int]]): List of clusters, each a set of vertex indices.\n",
    "        adj (Dict[int, Set[int]]): Vertex adjacency list.\n",
    "\n",
    "    Returns:\n",
    "        List[Set[int]]: List of sets, each containing the edge vertex indices for that cluster.\n",
    "    \"\"\"\n",
    "    cluster_edge_points = []\n",
    "\n",
    "    for cluster in clusters:\n",
    "        visited = set()\n",
    "        edge_points = set()\n",
    "        queue = deque()\n",
    "\n",
    "        # Initialize BFS with any point in the cluster\n",
    "        if not cluster:\n",
    "            cluster_edge_points.append(set())\n",
    "            continue\n",
    "        start = next(iter(cluster))\n",
    "        queue.append(start)\n",
    "        visited.add(start)\n",
    "\n",
    "        while queue:\n",
    "            vi = queue.popleft()\n",
    "            for vj in adj[vi]:\n",
    "                if vj not in cluster:\n",
    "                    edge_points.add(vi)\n",
    "                elif vj not in visited:\n",
    "                    visited.add(vj)\n",
    "                    queue.append(vj)\n",
    "\n",
    "        cluster_edge_points.append(edge_points)\n",
    "\n",
    "    return cluster_edge_points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cdf2cc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_points_per_cluster = find_cluster_edge_points(merged, adj)\n",
    "# Combine all edge points from all clusters\n",
    "all_edge_points = set()\n",
    "for edge_set in edge_points_per_cluster:\n",
    "    all_edge_points.update(edge_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d008248d",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [\n",
    "    'red', 'green', 'blue', 'yellow', 'magenta', 'cyan',\n",
    "    'orange', 'lime', 'deeppink', 'deepskyblue', 'gold',\n",
    "    'indigo', 'teal', 'crimson', 'mediumvioletred',\n",
    "    'chartreuse', 'orangered', 'darkturquoise',\n",
    "    'slateblue', 'darkgoldenrod'\n",
    "]\n",
    "\n",
    "plotter = pv.Plotter(shape=(1, 2))\n",
    "    \n",
    "plotter.subplot(0, 0)\n",
    "plotter.add_title(\"Remove Noise Faces\")\n",
    "plotter.add_mesh(smoothed_mesh, show_edges=True)\n",
    "for i, cluster in enumerate(merged):\n",
    "    merged_indices = list(cluster)\n",
    "    color = colors[i % len(colors)]\n",
    "    plotter.add_mesh(smoothed_mesh.points[merged_indices], color=color, point_size=10)\n",
    "\n",
    "plotter.subplot(0, 1)\n",
    "# Extract coordinates of all edge points\n",
    "plotter.add_title(\"Edges\")\n",
    "edge_coords = smoothed_mesh.points[list(all_edge_points)]\n",
    "point_cloud = pv.PolyData(edge_coords)\n",
    "# Plot edge points\n",
    "plotter.add_mesh(point_cloud, color='red', point_size=5)\n",
    "\n",
    "\n",
    "plotter.link_views()\n",
    "plotter.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6940c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nbformat\n",
    "\n",
    "# # üìå B∆Ø·ªöC 1: ƒê·∫∑t t√™n file notebook v√† file xu·∫•t ra\n",
    "# notebook_file = \"pyvista_learning.ipynb\"     # üëâ ƒë·ªïi th√†nh t√™n notebook c·ªßa b·∫°n\n",
    "# output_file = \"extract_code_from_ipynb.py\"      # üëâ t√™n file .py ƒë·ªÉ l∆∞u code\n",
    "\n",
    "# # üìå B∆Ø·ªöC 2: ƒê·ªçc file notebook\n",
    "# with open(notebook_file, \"r\", encoding=\"utf-8\") as f:\n",
    "#     nb = nbformat.read(f, as_version=4)\n",
    "\n",
    "# # üìå B∆Ø·ªöC 3: Gi·∫£ s·ª≠ √¥ hi·ªán t·∫°i ƒëang ch·∫°y l√† √¥ cu·ªëi c√πng (c√≥ th·ªÉ ƒëi·ªÅu ch·ªânh n·∫øu c·∫ßn)\n",
    "# current_cell_index = len(nb.cells) - 1\n",
    "\n",
    "# # üìå B∆Ø·ªöC 4: L·∫•y code t·ª´ t·∫•t c·∫£ c√°c cell code ph√≠a tr√™n\n",
    "# code_above = \"\"\n",
    "# for i in range(current_cell_index):\n",
    "#     cell = nb.cells[i]\n",
    "#     if cell.cell_type == \"code\":\n",
    "#         code_above += cell.source + \"\\n\\n\"\n",
    "\n",
    "# # üìå B∆Ø·ªöC 5: L∆∞u v√†o file .py\n",
    "# with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "#     f.write(code_above)\n",
    "\n",
    "# print(f\"‚úÖ ƒê√£ l∆∞u m√£ t·ª´ c√°c cell ph√≠a tr√™n v√†o file: {output_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Project3D",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
