{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46406998",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# import pyvista as pv\n",
    "from collections import defaultdict, deque\n",
    "import copy\n",
    "\n",
    "# ----------- Normal vectors estimation functions -----------\n",
    "\n",
    "def compute_face_normals_and_areas(mesh):\n",
    "    \"\"\"\n",
    "    Computes normal and area for each triangle face in the mesh.\n",
    "\n",
    "    Returns:\n",
    "        normals (np.ndarray): An (F, 3) array of unit normal vectors for each face.\n",
    "        areas (np.ndarray): A (F,) array of areas for each face.\n",
    "\n",
    "    Method:\n",
    "        - Extract vertex indices for each triangle face.\n",
    "        - Compute two edge vectors of the triangle.\n",
    "        - Use cross product to get the face normal (not normalized).\n",
    "        - Normalize to get unit normals.\n",
    "        - Compute triangle area as half of the magnitude of the cross product.\n",
    "    \"\"\"\n",
    "    faces = mesh.faces.reshape((-1, 4))[:, 1:]\n",
    "    v0 = mesh.points[faces[:, 0]]\n",
    "    v1 = mesh.points[faces[:, 1]]\n",
    "    v2 = mesh.points[faces[:, 2]]\n",
    "    \n",
    "    cross = np.cross(v1 - v0, v2 - v0)\n",
    "    areas = 0.5 * np.linalg.norm(cross, axis=1)\n",
    "    normals = cross / np.maximum(np.linalg.norm(cross, axis=1, keepdims=True), 1e-8)\n",
    "    return normals, areas\n",
    "\n",
    "def build_vertex_to_faces_map(mesh):\n",
    "    \"\"\"\n",
    "    Builds a mapping from each vertex index to the list of face indices that include that vertex.\n",
    "\n",
    "    The same with pyvista's mesh.point_cell_ids()\n",
    "    Returns:\n",
    "        defaultdict(list): A dictionary where each key is a vertex index, and the value is \n",
    "        a list of face indices (from mesh.faces) that contain the vertex.\n",
    "\n",
    "    Example:\n",
    "        {\n",
    "            0: {1, 5, 10},  # vertex 0 is part of faces 1, 5, and 10\n",
    "            1: {0, 2},      # vertex 1 is part of faces 0 and 2\n",
    "            ...\n",
    "        }    \n",
    "    \"\"\"\n",
    "    vertex_faces = defaultdict(list)\n",
    "    faces = mesh.faces.reshape((-1, 4))[:, 1:]\n",
    "    for i, (v1, v2, v3) in enumerate(faces):\n",
    "        vertex_faces[v1].append(i)\n",
    "        vertex_faces[v2].append(i)\n",
    "        vertex_faces[v3].append(i)\n",
    "    return vertex_faces\n",
    "\n",
    "def compute_area_weighted_vertex_normals(mesh):\n",
    "    \"\"\"\n",
    "    Computes unit vertex normals by averaging adjacent face normals, weighted by face area.\n",
    "\n",
    "    Returns:\n",
    "        vertex_normals (np.ndarray): An (N, 3) array of unit normal vectors for each vertex.\n",
    "\n",
    "    Method:\n",
    "        1. Compute face normals and face areas using cross products.\n",
    "        2. Build a mapping from each vertex to the list of adjacent face indices.\n",
    "        3. For each vertex:\n",
    "            - Retrieve adjacent faces.\n",
    "            - Average their normals, weighted by face area.\n",
    "            - Normalize the result to obtain a unit normal.\n",
    "    \"\"\"\n",
    "    face_normals, face_areas = compute_face_normals_and_areas(mesh)\n",
    "    vertex_faces = build_vertex_to_faces_map(mesh)\n",
    "    num_vertices = mesh.points.shape[0]\n",
    "    vertex_normals = np.zeros((num_vertices, 3))\n",
    "\n",
    "    for vi in range(num_vertices):\n",
    "        adjacent_faces = vertex_faces[vi]\n",
    "        if not adjacent_faces:\n",
    "            continue\n",
    "        areas = face_areas[adjacent_faces]\n",
    "        normals = face_normals[adjacent_faces]\n",
    "        weighted_sum = np.sum(normals * areas[:, np.newaxis], axis=0)\n",
    "        vertex_normals[vi] = weighted_sum / np.linalg.norm(weighted_sum)\n",
    "    \n",
    "    return vertex_normals\n",
    "\n",
    "# -------------------------- Normal vector angle --------------\n",
    "\n",
    "def normal_vector_angle(n_p, n_q, degrees=False):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    - n_p, n_q: 3D vectors (can be non-normalized).\n",
    "    - degrees: If True, return angle in degrees. Otherwise, radians.\n",
    "\n",
    "    Returns:\n",
    "    - Angle between vectors\n",
    "    \"\"\"\n",
    "    dot_product = np.dot(n_p, n_q)\n",
    "    norm_p = np.linalg.norm(n_p)\n",
    "    norm_q = np.linalg.norm(n_q)\n",
    "\n",
    "    if norm_p == 0 or norm_q == 0:\n",
    "        return 0.0\n",
    "\n",
    "    cos_theta = np.clip(dot_product / (norm_p * norm_q), -1.0, 1.0)\n",
    "    angle_rad = np.arccos(cos_theta)\n",
    "    return np.degrees(angle_rad) if degrees else angle_rad\n",
    "\n",
    "# ----------------------------------------------\n",
    "def sorting_flat_vertices_with_flatness(normals, adjacency):\n",
    "    \"\"\"\n",
    "    Find all flat vertices, compute their flatness (average normal angle to neighbors),\n",
    "    and return them sorted by flatness (ascending).\n",
    "    \n",
    "    Returns:\n",
    "        List of (vertex_index, flatness_value) tuples, sorted by flatness_value.\n",
    "    \"\"\"\n",
    "    flat_vertices = []\n",
    "    for vi, neighbors in adjacency.items():\n",
    "        if not neighbors:\n",
    "            continue\n",
    "        # Check if all neighbor angles are below threshold\n",
    "        angles = [normal_vector_angle(normals[vi], normals[vj]) for vj in neighbors]\n",
    "        flatness = sum(angles) / len(angles)\n",
    "        flat_vertices.append((vi, flatness))\n",
    "        # if all(angle < threshold for angle in angles):\n",
    "        #     flatness = sum(angles) / len(angles)\n",
    "        #     flat_vertices.append((vi, flatness))\n",
    "        \n",
    "    # Sort by flatness value (ascending)\n",
    "    flat_vertices.sort(key=lambda x: x[1])\n",
    "    return flat_vertices\n",
    "\n",
    "def cluster_surface(adj, v_normals, flat_v_sorted, weight, threshold):\n",
    "    '''\n",
    "    Parameters:\n",
    "    adj: A dictionary where each key is a vertex index, and \n",
    "         the value is a set of its neighboring vertex indices.\n",
    "\n",
    "    v_normals: An (N, 3) array of unit normal vectors for each vertex.\n",
    "\n",
    "    flat_v_sorted: 1D list of vertex_index, sorted by flatness_value.\n",
    "\n",
    "    weight: parameter for similarity function\n",
    "\n",
    "    threshold: If similarity function < threshold: add vertex to a cluster\n",
    "    '''\n",
    "    flat_v_sorted_copy = copy.copy(flat_v_sorted)\n",
    "    unvisited = set(range(len(v_normals)))\n",
    "    clusters = []\n",
    "\n",
    "    while unvisited:\n",
    "        start = flat_v_sorted_copy.pop(0)\n",
    "        if start not in unvisited:\n",
    "            continue \n",
    "        cluster = set([start]) # Step 2 of the section 3's algorithm\n",
    "        queue = deque([start])\n",
    "        unvisited.remove(start)\n",
    "        while queue:\n",
    "            vi = queue.popleft()\n",
    "            vi_neighbors = adj[vi]\n",
    "            for vj in vi_neighbors:\n",
    "                # Average angle btw\n",
    "                avg_angle_vi = np.mean([normal_vector_angle(v_normals[vi], v_normals[vj_neighbor]) for vj_neighbor in adj[vj]])\n",
    "\n",
    "                # Compute average normal vector all vertices in cluster by summing and normalizing\n",
    "                normals_sum = np.sum([v_normals[vk] for vk in cluster], axis=0)\n",
    "                normals_normalize = normals_sum / np.maximum(np.linalg.norm(normals_sum), 1e-8)\n",
    "                cluster_normal = normals_normalize\n",
    "\n",
    "                avg_angle_cluster = np.mean([normal_vector_angle(cluster_normal, v_normals[vj_neighbor]) for vj_neighbor in adj[vj]])\n",
    "                # Proximity computation\n",
    "                proximity = weight * avg_angle_vi + (1 - weight) * avg_angle_cluster\n",
    "\n",
    "                if proximity < threshold:\n",
    "                    if vj in unvisited:\n",
    "                        cluster.add(vj)\n",
    "                        queue.append(vj)\n",
    "                        unvisited.remove(vj)\n",
    "\n",
    "        clusters.append(cluster)\n",
    "    return clusters\n",
    "\n",
    "def merge_noise_faces(clusters, vertex_normals, adjacency, K=10):\n",
    "    \"\"\"\n",
    "    Merge small (noise) clusters into neighboring real clusters based on normal similarity,\n",
    "    while avoiding merging adjacent noise clusters together.\n",
    "\n",
    "    Parameters:\n",
    "        clusters (List[Set[int]]): List of vertex index sets (each cluster).\n",
    "        vertex_normals (np.ndarray): (N, 3) array of vertex normals.\n",
    "        adjacency (dict): {vertex_index: set(neighboring vertex indices)}.\n",
    "        K (int): Max vertex count for a cluster to be considered noise.\n",
    "\n",
    "    Returns:\n",
    "        List[Set[int]]: Updated clusters with noise surfaces merged into real surfaces.\n",
    "    \"\"\"\n",
    "    # Separate clusters\n",
    "    real_clusters = [c for c in clusters if len(c) >= K]\n",
    "    noise_clusters = [c for c in clusters if len(c) < K]\n",
    "\n",
    "    # Precompute normals\n",
    "    def cluster_avg_normal(cluster):\n",
    "        normals = [vertex_normals[vi] for vi in cluster]\n",
    "        summed = np.sum(normals, axis=0)\n",
    "        return summed / np.linalg.norm(summed) if np.linalg.norm(summed) > 0 else summed\n",
    "\n",
    "    real_normals = [cluster_avg_normal(c) for c in real_clusters]\n",
    "    noise_normals = [cluster_avg_normal(c) for c in noise_clusters]\n",
    "\n",
    "    # Create index-to-cluster maps for fast lookup\n",
    "    vertex_to_real_cluster = {}\n",
    "    for idx, cluster in enumerate(real_clusters):\n",
    "        for v in cluster:\n",
    "            vertex_to_real_cluster[v] = idx\n",
    "\n",
    "    merged_indices = set()  # track merged noise cluster indices\n",
    "\n",
    "    for ni, noise in enumerate(noise_clusters):\n",
    "        if ni in merged_indices:\n",
    "            continue\n",
    "        noise_normal = noise_normals[ni]\n",
    "\n",
    "        # Find adjacent real clusters only\n",
    "        neighbor_real_candidates = set()\n",
    "        for vi in noise:\n",
    "            for vj in adjacency[vi]:\n",
    "                if vj in vertex_to_real_cluster:\n",
    "                    neighbor_real_candidates.add(vertex_to_real_cluster[vj])\n",
    "\n",
    "        # Find the best matching real cluster by normal vector angle\n",
    "        best_real_idx = None\n",
    "        best_angle = float('inf')\n",
    "\n",
    "        for r_idx in neighbor_real_candidates:\n",
    "            angle = normal_vector_angle(noise_normal, real_normals[r_idx])\n",
    "            if angle < best_angle:\n",
    "                best_real_idx = r_idx\n",
    "                best_angle = angle\n",
    "\n",
    "        # Merge into best-matching real cluster\n",
    "        if best_real_idx is not None:\n",
    "            real_clusters[best_real_idx].update(noise)\n",
    "            merged_indices.add(ni)\n",
    "        # else: discard the noise cluster (could also be kept separately)\n",
    "\n",
    "    return real_clusters\n",
    "\n",
    "\n",
    "'''\n",
    "    This code implement the Laplace smoothing algorithm mentioned in the section 2 of the paper \n",
    "    \"Robust surface segmentation and edge feature lines extraction from fractured\n",
    "    ragments of relics\" \n",
    "'''\n",
    "\n",
    "\n",
    "# ---------- Laplacian Smoothing Functions ----------\n",
    "\n",
    "def build_vertex_adjacency(faces):\n",
    "    \"\"\"\n",
    "    Builds a dictionary where each key is a vertex index, and the value is a set of its neighboring vertex indices.\n",
    "\n",
    "    The same with pyvista's mesh.point_neighbors()\n",
    "    \"\"\"\n",
    "    adjacency = defaultdict(set)\n",
    "    for three_vertices in faces:\n",
    "        i, j, k = three_vertices\n",
    "        adjacency[i].update([j, k])\n",
    "        adjacency[j].update([i, k])\n",
    "        adjacency[k].update([i, j])\n",
    "    return adjacency\n",
    "\n",
    "def weighted_laplacian_smoothing(vertices, faces, iterations=1):\n",
    "    vertices = vertices.copy()\n",
    "    adjacency = build_vertex_adjacency(faces)\n",
    "\n",
    "    for _ in range(iterations):\n",
    "        new_vertices = vertices.copy()\n",
    "        for i in range(len(vertices)):\n",
    "            neighbors = list(adjacency[i])\n",
    "            if not neighbors:\n",
    "                continue\n",
    "\n",
    "            weights = []\n",
    "            for j in neighbors:\n",
    "                dist = np.linalg.norm(vertices[i] - vertices[j])\n",
    "                weight = 1.0 / (dist + 1e-8)  # Avoid division by zero\n",
    "                weights.append(weight)\n",
    "            weights = np.array(weights)\n",
    "            weights /= np.sum(weights)  # Normalize Î»_ij\n",
    "\n",
    "            weighted_sum = sum(weights[k] * (vertices[neighbors[k]] - vertices[i]) for k in range(len(neighbors)))\n",
    "            new_vertices[i] = vertices[i] + weighted_sum\n",
    "\n",
    "        vertices = new_vertices\n",
    "\n",
    "    return vertices\n",
    "\n",
    "def compute_mesh_smoothness(vertices, faces): # To see how much a mesh is smooth\n",
    "    adjacency = build_vertex_adjacency(faces)\n",
    "    smoothness_vals = []\n",
    "\n",
    "    for i in range(len(vertices)):\n",
    "        neighbors = list(adjacency[i])\n",
    "        if not neighbors:\n",
    "            continue\n",
    "        avg_neighbor = np.mean(vertices[neighbors], axis=0)\n",
    "        smoothness = np.linalg.norm(vertices[i] - avg_neighbor)\n",
    "        smoothness_vals.append(smoothness)\n",
    "\n",
    "    return np.mean(smoothness_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bcf475f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def convert_to_builtin_int(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        return {convert_to_builtin_int(k): convert_to_builtin_int(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_to_builtin_int(i) for i in obj]\n",
    "    elif isinstance(obj, set):\n",
    "        return {convert_to_builtin_int(i) for i in obj}\n",
    "    elif isinstance(obj, tuple):\n",
    "        return tuple(convert_to_builtin_int(i) for i in obj)\n",
    "    elif isinstance(obj, np.integer):  # Catch np.int64, np.int32, ...\n",
    "        return int(obj)\n",
    "    else:\n",
    "        return obj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9328c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pyvista as pv\n",
    "\n",
    "mesh = pv.read('D:/Learn_and_Study/USTH/Bachelor/3D_Project/CG_dataset/brick_part01.obj')\n",
    "# mesh = pv.examples.download_bunny()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "334092ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "faces_1D = mesh.faces\n",
    "faces_nD = faces_1D.reshape((-1, 4))[:, 1:]\n",
    "\n",
    "smoothed_vertices = weighted_laplacian_smoothing(mesh.points, faces_nD, 3)\n",
    "smoothed_mesh = pv.PolyData(smoothed_vertices, faces_1D)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56d2653e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vertices = smoothed_mesh.points \n",
    "faces_1D = smoothed_mesh.faces\n",
    "faces_nD = smoothed_mesh.faces.reshape((-1, 4))[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23db0ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "adj = build_vertex_adjacency(faces_nD)\n",
    "adj = convert_to_builtin_int(adj)\n",
    "\n",
    "v_normals = compute_area_weighted_vertex_normals(smoothed_mesh)\n",
    "\n",
    "flat_v_sorted = sorting_flat_vertices_with_flatness(v_normals, adj)\n",
    "flat_v_sorted = [vertex for vertex, flatness in flat_v_sorted]\n",
    "flat_v_sorted = convert_to_builtin_int(flat_v_sorted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1ccb0a",
   "metadata": {},
   "source": [
    "### Test cluster_surface()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eee7ff0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = cluster_surface(adj, v_normals, flat_v_sorted, 0.5, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8df7a0",
   "metadata": {},
   "source": [
    "### Test merge_noise_face() to remove noise faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e5eda6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = merge_noise_faces(clusters, v_normals, adj, K=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fca1de99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70d43008a817454f99b6252431ba1421",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Widget(value='<iframe src=\"http://localhost:50797/index.html?ui=P_0x189b98c4680_0&reconnect=auto\" class=\"pyvisâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "colors = [\n",
    "    'red', 'green', 'blue', 'yellow', 'magenta', 'cyan',\n",
    "    'orange', 'lime', 'deeppink', 'deepskyblue', 'gold',\n",
    "    'indigo', 'teal', 'crimson', 'mediumvioletred',\n",
    "    'chartreuse', 'orangered', 'darkturquoise',\n",
    "    'slateblue', 'darkgoldenrod'\n",
    "]\n",
    "\n",
    "plotter = pv.Plotter(shape=(1, 3))\n",
    "\n",
    "plotter.subplot(0, 0)\n",
    "plotter.add_title(\"Original\")\n",
    "plotter.add_mesh(smoothed_mesh, show_edges=True)\n",
    "\n",
    "plotter.subplot(0, 1)\n",
    "plotter.add_title(\"Clusters\")\n",
    "plotter.add_mesh(smoothed_mesh, show_edges=True)\n",
    "for i, cluster in enumerate(clusters):\n",
    "    cluster_indices = list(cluster)\n",
    "    color = colors[i % len(colors)]\n",
    "    plotter.add_mesh(smoothed_mesh.points[cluster_indices], color=color, point_size=10)\n",
    "\n",
    "\n",
    "plotter.subplot(0, 2)\n",
    "plotter.add_title(\"Merged\")\n",
    "plotter.add_mesh(smoothed_mesh, show_edges=True)\n",
    "for i, cluster in enumerate(merged):\n",
    "    merged_indices = list(cluster)\n",
    "    color = colors[i % len(colors)]\n",
    "    plotter.add_mesh(smoothed_mesh.points[merged_indices], color=color, point_size=10)\n",
    "\n",
    "plotter.link_views()\n",
    "plotter.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b89532",
   "metadata": {},
   "source": [
    "### Test surface differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc467545",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_local_bending_energy(vertices, vertex_normals, adjacency):\n",
    "    \"\"\"\n",
    "    Computes local bending energy e_k(p) for each vertex p as defined in the paper.\n",
    "\n",
    "    Parameters:\n",
    "        vertices (np.ndarray): (N, 3) array of vertex positions.\n",
    "        vertex_normals (np.ndarray): (N, 3) array of vertex normals.\n",
    "        adjacency (dict): {vertex_index: set of neighbor vertex indices}\n",
    "\n",
    "    Returns:\n",
    "        e_k (np.ndarray): (N,) array of local bending energy per vertex.\n",
    "    \"\"\"\n",
    "    N = len(vertices)\n",
    "    e_k = np.zeros(N)\n",
    "\n",
    "    for p in range(N):\n",
    "        neighbors = adjacency[p]\n",
    "        if not neighbors:\n",
    "            e_k[p] = 0\n",
    "            continue\n",
    "\n",
    "        total = 0.0\n",
    "        for q in neighbors:\n",
    "            norm_diff = np.linalg.norm(vertex_normals[p] - vertex_normals[q]) ** 2\n",
    "            geom_dist = np.linalg.norm(vertices[p] - vertices[q]) ** 2 + 1e-8  # avoid div by zero\n",
    "            total += norm_diff / geom_dist\n",
    "\n",
    "        e_k[p] = total / len(neighbors)\n",
    "\n",
    "    return e_k\n",
    "\n",
    "from scipy.spatial import cKDTree\n",
    "import numpy as np\n",
    "\n",
    "def compute_integrated_bending_energy_fast(e_k, vertices, radius):\n",
    "    \"\"\"\n",
    "    Fast computation of e_{k,r}(p) using cKDTree for efficient radius search.\n",
    "    \"\"\"\n",
    "    N = len(vertices)\n",
    "    e_kr = np.zeros(N)\n",
    "\n",
    "    # Build KDTree for fast radius neighbor queries\n",
    "    tree = cKDTree(vertices)\n",
    "\n",
    "    # Query all neighbors within radius r\n",
    "    all_neighbors = tree.query_ball_point(vertices, r=radius)\n",
    "\n",
    "    for i in range(N):\n",
    "        neighbors = all_neighbors[i]\n",
    "        if not neighbors:\n",
    "            e_kr[i] = e_k[i]  # fallback if no neighbors\n",
    "        else:\n",
    "            e_kr[i] = np.mean(e_k[neighbors])\n",
    "    print(all_neighbors)\n",
    "    return e_kr\n",
    "\n",
    "# def classify_faces_by_roughness(mesh, e_kr, threshold=0.05):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e59470",
   "metadata": {},
   "outputs": [],
   "source": [
    "e_k = compute_local_bending_energy(vertices, v_normals, adj)\n",
    "e_kr = compute_integrated_bending_energy_fast(e_k, vertices, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e1f89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the index of the face color\n",
    "\n",
    "# Fractured surface\n",
    "magenta_index = colors.index('magenta')  # 4\n",
    "magenta_points = list(clusters[magenta_index]) if magenta_index < len(clusters) else []\n",
    "\n",
    "cyan_index = colors.index('cyan')  # 4\n",
    "cyan_points = list(clusters[cyan_index]) if cyan_index < len(clusters) else []\n",
    "\n",
    "# Original surface\n",
    "red_index = colors.index('red')  # 4\n",
    "red_points = list(clusters[red_index]) if red_index < len(clusters) else []\n",
    "\n",
    "blue_index = colors.index('blue')  # 4\n",
    "blue_points = list(clusters[blue_index]) if blue_index < len(clusters) else []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5cb678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(magenta_points))\n",
    "# print(len(cyan_points))\n",
    "# print(len(red_points))\n",
    "# print(len(blue_points))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14716a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# original_indices = red_points[:50] + blue_points[:50]\n",
    "# fractured_indices = cyan_points[:50] + magenta_points[:50]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57eed03",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_indices = merged[0][:50] + merged[2][:50]\n",
    "fractured_indices = merged[5][:50] + merged[4][:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceefe8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_values = e_kr[original_indices]\n",
    "fractured_values = e_kr[fractured_indices]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(original_values, bins=30, alpha=0.6, label='Original')\n",
    "plt.hist(fractured_values, bins=30, alpha=0.6, label='Fractured')\n",
    "plt.axvline(x=np.mean(original_values), color='blue', linestyle='--')\n",
    "plt.axvline(x=np.mean(fractured_values), color='red', linestyle='--')\n",
    "plt.legend()\n",
    "plt.title(\"Distribution of e_{k,r}(p)\")\n",
    "plt.xlabel(\"Integrated Bending Energy\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278daac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fractured_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a917a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "mu1, std1 = np.mean(original_values), np.std(original_values)\n",
    "mu2, std2 = np.mean(fractured_values), np.std(fractured_values)\n",
    "\n",
    "# Solve for x in norm1(x) == norm2(x)\n",
    "a = 1/(2*std1**2) - 1/(2*std2**2)\n",
    "b = mu2/(std2**2) - mu1/(std1**2)\n",
    "c = (mu1**2)/(2*std1**2) - (mu2**2)/(2*std2**2) - np.log(std2/std1)\n",
    "\n",
    "discriminant = b**2 - 4*a*c\n",
    "x1 = (-b + np.sqrt(discriminant)) / (2*a)\n",
    "x2 = (-b - np.sqrt(discriminant)) / (2*a)\n",
    "\n",
    "# Choose the threshold between the two means\n",
    "threshold = x1 if mu1 < x1 < mu2 or mu2 < x1 < mu1 else x2\n",
    "threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6940c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nbformat\n",
    "\n",
    "# # ðŸ“Œ BÆ¯á»šC 1: Äáº·t tÃªn file notebook vÃ  file xuáº¥t ra\n",
    "# notebook_file = \"pyvista_learning.ipynb\"     # ðŸ‘‰ Ä‘á»•i thÃ nh tÃªn notebook cá»§a báº¡n\n",
    "# output_file = \"extract_code_from_ipynb.py\"      # ðŸ‘‰ tÃªn file .py Ä‘á»ƒ lÆ°u code\n",
    "\n",
    "# # ðŸ“Œ BÆ¯á»šC 2: Äá»c file notebook\n",
    "# with open(notebook_file, \"r\", encoding=\"utf-8\") as f:\n",
    "#     nb = nbformat.read(f, as_version=4)\n",
    "\n",
    "# # ðŸ“Œ BÆ¯á»šC 3: Giáº£ sá»­ Ã´ hiá»‡n táº¡i Ä‘ang cháº¡y lÃ  Ã´ cuá»‘i cÃ¹ng (cÃ³ thá»ƒ Ä‘iá»u chá»‰nh náº¿u cáº§n)\n",
    "# current_cell_index = len(nb.cells) - 1\n",
    "\n",
    "# # ðŸ“Œ BÆ¯á»šC 4: Láº¥y code tá»« táº¥t cáº£ cÃ¡c cell code phÃ­a trÃªn\n",
    "# code_above = \"\"\n",
    "# for i in range(current_cell_index):\n",
    "#     cell = nb.cells[i]\n",
    "#     if cell.cell_type == \"code\":\n",
    "#         code_above += cell.source + \"\\n\\n\"\n",
    "\n",
    "# # ðŸ“Œ BÆ¯á»šC 5: LÆ°u vÃ o file .py\n",
    "# with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "#     f.write(code_above)\n",
    "\n",
    "# print(f\"âœ… ÄÃ£ lÆ°u mÃ£ tá»« cÃ¡c cell phÃ­a trÃªn vÃ o file: {output_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Project3D",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
